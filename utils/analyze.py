import time
from llama_cpp import Llama
from datetime import datetime
import json
from llm_promt import build_gguf_resume_prompt
# === Configuration ===
MODEL_PATH = r"models\Llama-3.2-3B-Instruct-UD-Q4_K_XL.gguf"  # Change as needed

def analyze_resume(resume_text, mandatory_skills, optional_skills):
    gguf_prompt = build_gguf_resume_prompt(resume_text, mandatory_skills, optional_skills)

    N_TOKENS = 1024
    N_THREADS = 16  # Set based on your CPU (e.g. os.cpu_count())

    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=2048,
        n_threads=N_THREADS
    )

    print("Generating response...")
    start_infer = time.time()

    response = llm(
        prompt=gguf_prompt,
        max_tokens=N_TOKENS,
        temperature=0,
    )

    infer_time = time.time() - start_infer
    output_text = response["choices"][0]["text"]

    # === Output ===
    response_dict = json.loads(output_text.strip())

    print("\n--- Stats ---")
    print(f"üß† Prompt Tokens: {response['usage']['prompt_tokens']}")
    print(f"‚úçÔ∏è  Response Tokens: {response['usage']['completion_tokens']}")
    print(f"‚ö° Generation Time: {infer_time:.2f} seconds")
    print(f"‚ö° Tokens/sec: {response['usage']['completion_tokens'] / infer_time:.2f}")

    return response_dict